\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Defining Vector Spaces and Subspaces}

We've seen the sets \( \mathbb{R}^2 \) and \( \mathbb{R}^3 \)
where the elements are vectors as translations in plane and 3-space.
These sets have an algebraic structure.

\begin{definition}[Vector Space]
    Any set following axioms A1-A10 is called a \textbf{vector space}.

    Given a vector space \( A \), vectors \( u,v,w \in A \) and scalars \( r,s \):
    \begin{enumerate}[label=A\arabic*.]
        \item We can define addition (and the space is closed under addition).
        \item We can define scalar multiplication (and the space is closed under scalar multiplication).
        \item \( u+v = v+u \)
        \item \( (u+v)+w = u+(v+w) \)
        \item There exists an additive identity in \( A \).
        \item There exists an additive inverse for any \( u \in A \).
        \item There exists a multiplicative identity in \( A \).
        \item \( (rs)v = r(sv) \)
        \item \( r(u+v) = ru + rv \)
        \item \( (r+s)v = rv + sv \)
    \end{enumerate}
\end{definition}

By studying vector spaces, we can develop algebraic machinery to apply to all kinds of other objects.

\begin{example}[]
    \( Q = \left\{ a_0 + a_1 x + a_2 x^2 \mid a_i \in \mathbb{R} \right\} \)
    (Polynomials of degree 2 or less)
    
    This is a vector space!
    \begin{enumerate}[label=A\arabic*.]
        \item Adding doesn't increase the degree. \checkmark
        \item Scalar multiplication similarly doesn't increase the degree. \checkmark
        \item Addition in \( \mathbb{R} \) is commutative. \checkmark
        \item By the associative property of \( \mathbb{R} \),
        \begin{align*}
            \Bigl( (a_0 + a_1 x + a_2 x^2) + (b_0 + b_1 x + b_2 x^2) \Bigr) + (c_0 + c_1 x + c_2 x^2) \\
            = \Bigl( (a_0 + b_0) + c_0 \Bigr) + \Bigl( (a_1 + b_1) + c_1 \Bigr)x + \Bigl( (a_2 + b_2) + c_2 \Bigr)x^2 \\
            = \Bigl( a_0 + (b_0 + c_0) \Bigr) + \Bigl( a_1 + (b_1 + c_1) \Bigr)x + \Bigl( a_2 + (b_2 + c_2) \Bigr)x^2 \\
            = a_0 + a_1 x + a_2 x^2 + (b_0 + c_0) + (b_1 + c_1)x + (b_2 + c_2)x^2 \checkmark
        \end{align*}
        \item 0 polynomial \checkmark
        \item \( -(a_0 + a_1 x + a_2 x^2) = -a_0 - a_1x - a_2x^2 \) \checkmark
        \item \( 1\cdot(a_0 + a_1 x + a_2 x^2) = a_0 + a_1 x + a_2 x^2 \) \checkmark
        \item \checkmark
        \item \checkmark
        \item \checkmark
    \end{enumerate}
\end{example}

\begin{example}
    \( V = \mathbb{R}^2 \) \\
    \( S = \left\{ (x,y) \mid x^2-y^2 = 0 \right\} \) \\
    Is \( S \) a vector space?

    No, since addition is not closed.
\end{example}

\begin{example}
    \( V = M_{3 \times 2}(\mathbb{R}) =\left\{ \begin{bmatrix} a & d \\ b & e \\ c & f \end{bmatrix} \Bigg\vert \; a,b,c,d,e,f \in \mathbb{R} \right\} \) \\
    \( S = \left\{ A \in V \mid \text{columns each sum to 0} \right\} \) \\
    Is \( S \) a vector space?
    
    For example, \( \begin{bmatrix} 1 & 0 \\ 0 & 2 \\ -1 & -2 \end{bmatrix} \in S \).
    Now, \( \vec{0} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \\ 0 & 0 \end{bmatrix} \).

    Closed under scalar mult? \checkmark \\
    Consider \( \begin{bmatrix} a & d \\ b & e \\ c & f \end{bmatrix} \text{ s.t. } a+b+c = 0 \text{ and } d+e+f=0 \). \\
    Then \( m\begin{bmatrix} a & d \\ b & e \\ c & f \end{bmatrix} = \begin{bmatrix} ma & md \\ mb & me \\ mc & mf \end{bmatrix} \). \\
    So \( ma+mb+mc = m(a+b+c) = m(0) \) and \( md+me+mf = m(d+e+f) = m(0) \). \checkmark

    Closed under addition? \checkmark \\
    Consider \( \begin{bmatrix} a & d \\ b & e \\ c & f \end{bmatrix}, \; \begin{bmatrix} a' & d' \\ b' & e' \\ c' & f' \end{bmatrix} \in S\). \\
    Now \( \begin{bmatrix} a & d \\ b & e \\ c & f \end{bmatrix} + \begin{bmatrix} a' & d' \\ b' & e' \\ c' & f' \end{bmatrix} =
    \begin{bmatrix} a+a' & d+d' \\ b+b' & e+e' \\ c+c' & f+f' \end{bmatrix} \) \\
    Check \( (a+a') + (b+b') + (c+c') = (a+b+c) + (a'+b'+c') = 0 \).
\end{example}

\begin{note}
    Since the above examples of \( S \) are subsets of known vector spaces, we just have to check:
    \begin{enumerate}
        \item Closed under \( +,- \)
        \item Closed under scalar multiplication
    \end{enumerate}

    These are called \textbf{subspaces}.
\end{note}

Vectors spaces do not just include \( \mathbb{R}^n \text{ and } \mathbb{C}^n \).
We have:
\begin{itemize}
    \item \( C^k(I) \): function on an internal \( I \) with \( k \) continuous derivatives
    \item \( P_n(\mathbb{R}) \): polynomials with degree up to \( n \)
    \item \( M_{m \times n}(\mathbb{R}) \)
\end{itemize}

\begin{example}[]
    Consider \( y''+a_1(x)y' + a_2(x) = 0 \) on an interval \( I \). Let \( S \) be the set of solutions to this LODE.
    Is \( S \) a vector space?
    \begin{itemize}
        \item Addition: If \( y_1,y_2 \in S \), is \( y_1+y_2 \in S \)?
            \[ (y_1+y_2)''+a_1(x)(y_1+y_2)' + a_2(x)(y_1+y_2) \\
            = y_1''+y_2'' + a_1(x)(y_1'+y_2') + a_2(x)(y_1+y_2) \\
            = 0+0 \]
            Therefore, \( S \) is closed under addition.
        \item Scalar Multiplication: If \( y \in S \), is \( cy \in S \)?
            Yes since \( (cy)'' = cy'' \) and \( (cy)' = cy' \).
    \end{itemize}
    Therefore \( S \) is a vector space, a subspace of \( C^2(I) \).
\end{example}

\begin{example}[]
    Let \( V = \mathbb{R}^3 \) and \( S \) be the solutions to the linear system:
    \begin{align*}
        2x + 3y + z = 0 \\
        x + 2y + 3z = 0
    \end{align*}
    Then \( S \subset V \) is a subspace.
    All vectors in the subspace lie on a line through the origin contained on both planes.
\end{example}

Using this concept of subspaces, we can find solutions to certain differential equations or systems of linear equations.

\begin{definition}[Null Space]
    The solutions of a homogeneous linear system \( A \vec{x} = 0 \) is called the \textbf{null space} of \( A \), any \( A_{m \times n} \).
    It is a subspace of \( \mathbb{R}^n \) and is also known as the \textbf{kernel} of \( A \).
\end{definition}

For \( \mathbb{R}^3 \), subspaces consist of either a point {\( \vec{0} \)}, a line through \( \vec{0} \), or a plane through \( \vec{0} \).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Spanning Sets}

\begin{definition}[Span]
    A linear combination of vectors \( \vec{v_1}, \vec{v_2}, \dots, \vec{v_n} \in V \) is a vector
    \( \vec{v} = c_1\vec{v_1} + c_2\vec{v_2} + n\vec{v_n} \) in \( V \).

    The \textbf{span} of \( \left\{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_n} \right\} \) is the set of all linear combinations.
    The span is a subspace of the vector space \( V \).
\end{definition}

\begin{example}[]
    Compute the span of \( \left\{ (-4, 1, 3), \; (5,1,6), \; (6,0,2) \right\} \) in \( \mathbb{R}^3 \).

    We want the set \( \left\{ (a,b,c) \right\} \) that are linear combinations of the three vectors.
    In other words, the solutions of the linear system:
    \[ \begin{bmatrix}
        -4 & 5 & 6 & a \\
        1 & 1 & 0 & b \\
        3 & 6 & 2 & c
    \end{bmatrix} \]

    \begin{gather*}
        \xrightarrow[]{1. \; 2. \; 3.}
        \begin{bmatrix}
            1 & 1 & 0 & b \\
            0 & 9 & 6 & a+4b \\
            0 & 3 & 2 & c-3b
        \end{bmatrix}
        \xrightarrow[]{4. \; 5.}
        \begin{bmatrix}
            1 & 1 & 0 & b \\
            0 & 3 & 2 & c-3b \\
            0 & 0 & 0 & a+13b-3c
        \end{bmatrix}
    \end{gather*}
    \begin{enumerate}
        \item \( P_{12} \)
        \item \( A_{12}(4) \)
        \item \( A_{13}(-3) \)
        \item \( P_{23} \)
        \item \( A_{23}(-3) \)
    \end{enumerate}
    There is a solution if and only if \( \text{rk}A = \text{rk}A^\# \).
    So \( a+13b-3c = 0 \). We conclude the span is on a plane \( \boxed{x+13y-3z=0} \).
\end{example}

\begin{note}
    One of the three vectors is redundant.
\end{note}

\begin{example}[]
    Write \( (-4,1,3) \) as a linear combination of \( (5,1,6) \text{ and } (6,0,2) \).

    We want constants \( c_1,c_2 \) such that:
    \[ c_1 \begin{bmatrix} 5 \\ 1 \\ 6 \end{bmatrix} + c_2 \begin{bmatrix} 6 \\ 0 \\ 2 \end{bmatrix}
    = \begin{bmatrix} -4 \\ 1 \\ 3 \end{bmatrix} \]
    We need to solve \( \begin{bmatrix}
        5 & 6 & -4 \\
        1 & 0 & 1 \\
        6 & 2 & 3
    \end{bmatrix} \)
    \begin{gather*}
        \xrightarrow[2. \; 3.]{1.}
        \begin{bmatrix}
            1 & 0 & 1 \\
            0 & 6 & -9 \\
            0 & 2 & -3
        \end{bmatrix}
        \xrightarrow[]{4. \; 5.}
        \begin{bmatrix}
            1 & 0 & 1 \\
            0 & 2 & -3 \\
            0 & 0 & 0
        \end{bmatrix}
        \xrightarrow[]{6.}
        \begin{bmatrix}
            1 & 0 & 1 \\
            0 & 1 & \frac{-3}{2} \\
            0 & 0 & 0
        \end{bmatrix}
    \end{gather*}

    Thus \( \begin{bmatrix} 5 \\ 1 \\ 6 \end{bmatrix} -\displaystyle \frac{3}{2} \begin{bmatrix} 6 \\ 0 \\ 2 \end{bmatrix}
    = \begin{bmatrix} -4 \\ 1 \\ 3 \end{bmatrix} \).
\end{example}

The moral of this example is that sometimes a spanning set has redundant vectors.
We would like a spanning set to have no redundant vectors or a "minimally spanning set."

\begin{definition}[Linear Dependence]
    A set \( \left\{ \vec{v_1}, \vec{v_2}, \dots, \vec{v_n} \right\} \) in a vector space \( V \) is \textbf{linearly dependent}
    if there exists some \( c_1, c_2, \dots, c_n \) not all zero such that:
    \( c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} = \vec{0} \).
    (This is called the dependence relation, and we would say one of the vectors is redundant.)

    Otherwise, the set is \textbf{linearly independent} and the relation \( c_1\vec{v_1} + c_2\vec{v_2} + \cdots + c_n\vec{v_n} = \vec{0} \)
    implies \( c_1 = c_2 = \cdots = c_n = 0\).
\end{definition}

\begin{example}[]
    The vectors \(\vec{v_1} = \begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix},
    \; \vec{v_2} = \begin{bmatrix} -1 \\ 2 \\ -1 \end{bmatrix},
    \; \vec{v_3} = \begin{bmatrix} -1 \\ -1 \\ 2 \end{bmatrix} \) are linearly dependent. \\
    Find the dependence relation.

    We need to solve \( A\vec{x} = \vec{0} \) and \( \text{rk}A \leq 2 \) for there to be nontrivial solutions.
    \begin{gather*}
        \begin{bmatrix}
            2 & -1 & -1 & 0 \\
            -1 & 2 & -1 & 0 \\
            -1 & -1 & 2 & 0
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 1 & -2 & 0 \\
            0 & 3 & -3 & 0 \\
            0 & -3 & 3 & 0
        \end{bmatrix}
        \rightarrow
        \begin{bmatrix}
            1 & 1 & -2 & 0 \\
            0 & 1 & -1 & 0 \\
            0 & 0 & 0 & 0
        \end{bmatrix}
    \end{gather*}
    Parameterizing, we find that \( c_3 = t, \; c_2 = t, \; c_1 = 2t-t = t \).

    Therefore \( \boxed{\vec{v_1}+\vec{v_2}+\vec{v_3} = \vec{0}} \).
\end{example}

\subsection{Method for Testing Spans}

Given \( \left\{ \vec{v_1}, \, \dots \, , \vec{v_n} \right\} \), to determine if they are linearly dependent, form a matrix
\[ A = \begin{bmatrix}
    \vec{v_1} \; \vec{v_2} \; \dots \; \vec{v_n}
\end{bmatrix} \]
Then the set is linearly dependent if and only if \( A \vec{c} = \vec{0} \) has nontrivial solutions.
In other words, there exists some non-zero scalars \( c_1, \, \dots \, , c_k\) such that
\( c_1 \vec{v_1} + \cdots + c_k \vec{v_k} = \vec{0} \) if and only if \( \text{rk}A<k \).

The technique is to commit ERO's and find \( \text{rref}(A) \).

For example, suppose we have a span and \( \text{rref}(A^\#) \) is:
\[
\begin{bNiceMatrix}[last-row]
    1 & 2 & 0 & 3 & 0 & 0 \\
    0 & 0 & 1 & 4 & 0 & 0 \\
    0 & 0 & 0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 0 & 0 & 0 \\
    & \uparrow & & \uparrow & & 
\end{bNiceMatrix}
\]

We have two free variables.

\underline{Claim:} Each free variable gives a nontrivial dependence with bound variables.

So for this example, there are two dependence relations where \( \vec{v_2} \text{ and } \vec{v_4} \) are redundant
and \( \vec{v_1}, \; \vec{v_3}, \; \vec{v_5} \) form a linearly independent span.

In general, we can conclude that \( v_1, \, \dots \, , \vec{v_k} \) are linearly independent
if and only if there are no free variables in \( \text{rref}(A^\#) \) or equivalently \( \text{rk}A = k \).

\begin{theorem}[]
    In \( \mathbb{R}^n \), a set \( \left\{ \vec{v_1}, \, \dots \, , \vec{v_n} \right\} \) is linearly independent
    if and only if, for the matrix \( A = \left[ \vec{v_1}, \, \dots \, , \vec{v_n} \right] \), \( \det A \neq 0 \)
    or \( \text{rref}A = I_n \).
\end{theorem}

\begin{example}[]
    Find all values of k such that: \( \{ (1,1,0,-1), \, (1,k,1,1), \, (2,1,k,1), \, (-1,1,1,k)\} \) are linearly independent.

    Applying the previous theorem, the solution form is:
    \[
    \begin{bmatrix}
        1 & 1 & 2 & -1 \\
        1 & k & 1 & 1 \\
        0 & 1 & k & 1 \\
        -1 & 1 & 1 & k
    \end{bmatrix}
    \begin{bmatrix}
        c_1 \\ c_2 \\ c_3 \\ c_4
    \end{bmatrix}
    =
    \begin{bmatrix}
        0 \\ 0 \\ 0 \\ 0
    \end{bmatrix}
    \]

    Then \( \det A =  k^3 -2k^2 -5k +6 = (k-1)(k+2)(k-3) \).
    We conclude \( \boxed{k \neq -2, 1, -3} \).
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linear Independence of Functions}

\begin{definition}[Wronskian]
    Let \( f_1, f_2, \, \dots \, , f_n \) be in \( C^{n=1}(I) \).
    The \textbf{Wronskian} is:
    \[ W[f_1, f_2, \, \dots \, , f_n] = \begin{vmatrix}
        f_1 & f_2 & \dots & f_n \\
        f_1 & f_2 & \dots & f_n \\
        \vdots & \vdots & \ddots & \vdots \\
        f_1 & f_2 & \dots & f_n \\
    \end{vmatrix} \]
\end{definition}

Then if \( W[f_1, f_2, \, \dots \, , f_n](x) \neq 0 \) for some \( x \in I \), then the functions are linearly independent.

\underline{Sketch (of Proof)}

The dependence relation has the form: \[ c_1f_1 + c_2f_2 + \cdots + c_nf_n = 0 \text{ for all } x \]
This implies: \[ c_1f_1' + c_2f_2' + \cdots + c_nf_n' = 0 \text{ for all } x \] and so on, ending with
\[ c_1f_1^{n-1} + c_2f_2^{n-1} + \cdots + c_nf_n^{n-1} = 0 \text{ for all } x \].

We conclude that \( W[f_1, f_2, \, \dots \, , f_n](x) = 0 \text{ for all } x \).

Therefore if \( W[f_1, f_2, \, \dots \, , f_n](x) \neq 0 \), then \( c_1f_1 + c_2f_2 + \cdots + c_nf_n \neq 0 \text{ for some } x \).

\begin{example}[Exponentials]
    Determine whether \( e^x \) and \( e^{-x} \) are linearly independent on \( (-\infty,\infty) \).
    \[ W[e^x,e^{-x}] =
    \begin{vmatrix}
        e^x & e^{-x} \\
        e^x & -e^{-x}
    \end{vmatrix}
    = -1-1 = -2 \]
    We conclude they are linearly independent on \( (-\infty,\infty) \).

    What about \( e^x\) and \( e^{2x} \)?
    \[ W[e^x,e^{2x}] =
    \begin{vmatrix}
        e^x & e^{2x} \\
        e^x & 2e^{2x}
    \end{vmatrix}
    = e^{3x} \neq 0 \]
    We also conclude they are linearly independent.
\end{example}

\begin{example}[\( P_2(x) \)]
    Determine the linear independence of \( \{1,x,x^2\} \).
    \[ W[1,x,x^2] =
    \begin{vmatrix}
        1 & x & x^2 \\
        0 & 1 & 2x \\
        0 & 0 & 2
    \end{vmatrix}
    = 2 \]
    Therefore, \( \{1,x,x^2\} \) are linearly independent.

    What about \( \{1,x,x^2\} \)?
    \[ W[1,x,2x] =
    \begin{vmatrix}
        1 & x & 2x \\
        0 & 1 & 2 \\
        0 & 0 & 0
    \end{vmatrix}
    = 0 \]
    We cannnot draw a conclusion soley on the Wronskian, but look:
    \[ c_1 \cdot 1 + c_2 \cdot x + c_3 \cdot 2x = 0 \]
    \[ c_1 = 0, \, c_2 = -2, \, c_3 = 1 \]
    Therefore, \( \{1,x,x^2\} \) are linearly dependent.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bases and Dimensions}

Given any number of vectors \( \vec{v_1}, \, \vec{v_2}, \, \dots, \, \vec{v_k} \) in a vector space \( V \),
we can define a subspace of \( V \) as \( W = \text{span} \{ \vec{v_1}, \, \vec{v_2}, \, \dots, \, \vec{v_k} \} \)
and write \( W \subset V \).
Equivalently we can write \( W \) as \( W = \{\sum a_i \vec{i}: a_i \in \mathbb{R} \} \).

\begin{definition}[Basis]
    A \textbf{basis} for a vector space \( V \) is a set of vectors that:
    \begin{enumerate}
        \item span \( V \).
        \item are linearly independent.
    \end{enumerate}
\end{definition}

\begin{example}[]
    In \( \mathbb{R}^3 \), the standard basis is:
    \( \left\{
        \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix},
        \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix},
        \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
    \right\} \).

    In \( P_2 \), the standard basis is: \( \{1,x,x^2\} \).
\end{example}

\begin{note}
    These are not unique. (We can write multiple bases.)
\end{note}

\begin{theorem}[]
    Any two bases for \( V \) have the same number of vectors (assuming they are finite).
\end{theorem}

\underline{Sketch of Proof}

If we had two bases:
\[ \{ \vec{v_1,} \, \dots \, , \vec{v_m}\}, \; \{\vec{w_1}, \, \dots \, \vec{w_n}\} \]

Let say that \( m \leq n\) and \( A = [ \vec{v_1,} \, \dots \, , \vec{v_m} ] \).
Now \( A \vec{c} = \vec{w_j} \) has unique solutions for all \( j \) since \( \vec{v_i} \) span \( V \).
Since \( A \) has \( m \) columns, 
By Thm. of Rank, \( \text{rk}A = rk(A^\#) = n \).
*Finish*

Thus, from the previous theorem, we can define the following.

\begin{definition}[Dimension]
    The \textbf{dimension} of a vector space \( V \) is the number of elements in any basis.
\end{definition}

\begin{example}[]
    Find a basis for the subspace of \( \mathbb{R}^4 \) spanned by:
    \[
        \underline{\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}},
        \begin{bmatrix} 2 \\ 1 \\ 0 \\ 0 \end{bmatrix},
        \underline{\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}},
        \begin{bmatrix} 3 \\ 4 \\ 5 \\ 0 \end{bmatrix},
        \underline{\begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}}
    \].

    We can simply choose the underlined vectors. The dimension of this subspace is 3.
\end{example}

\subsection{Secret Trick}

Given a \( m \times n \) matrix \( A \),
it is useful to know:
\begin{itemize}
    \item A basis for \( \text{im}(A) \).
    \item A basis for \( \text{ker}(A) = \text{null}(A) \).
    \item A basis for the solution space for the linear system, \( A \textbf{c} = \textbf{0} \).
\end{itemize}
This section will detail a shorthand method to find all these bases quickly.

Let \( A = \begin{bmatrix}
    v_1 & v_2 & \dots & v_n
\end{bmatrix} \), an \( m \times n \) matrix.
First, compute \( \text{rref}(A) \) in the usual way. Suppose:
\[ \text{rref}(A) = \begin{bmatrix}
    0 & 1 & b_{13} & 0 & b_{15} & 0 \\
    0 & 0 & 0 & 1 & b_{25} & 0 \\
    0 & 0 & 0 & 0 & 0 & 1
\end{bmatrix} \]

The free-variable columns correspond to redundant vectors. The bound-variable columns correspond to a basis of the span.
Here, \( \textbf{v}_1, \textbf{v}_3, \text{ and } \textbf{v}_5 \) are redundant vectors,
and \( \{ \textbf{v}_2, \textbf{v}_4, \textbf{v}_6 \} \) are a basis for \( \text{im}(A) \).

The dependence-relation vector \( \textbf{c}_j \) corresponding to each free-variable column can be constructed by:
\begin{enumerate}
    \item Put 1 into the \textit{j}-th spot.
    \item If \( b_{ij} \neq 0 \), and the leading 1 is in the \textit{i}-th row is in column \( k < j \),
        put \( -b_{ij} \) in the \textit{k}-th spot.
    \item Put 0's everywhere else.
\end{enumerate}
Here, \( \textbf{c}_1 = \langle 1,0,0,0,0,0 \rangle, \,
    \textbf{c}_3 = \langle 0, -b_{13}, 1, 0, 0, 0 \rangle, \,
    \textbf{c}_5 = \langle 0, -b_{15}, 0, -b_{25}, 1, 0 \rangle \)

Recall that the solution space is equivalent to \( \text{ker}(A) = \text{null}(A) \).

\begin{example}[]
    Let \( A = \begin{bmatrix}
        1 & 2 & 3 & 2 & 3 \\
        3 & 6 & 9 & 6 & 2 \\
        1 & 2 & 4 & 1 & 2 \\
        2 & 4 & 9 & 1 & 2
    \end{bmatrix} \). Find bases for the \( \ima A \) and \( \ker A \).

    Now \( \rref(A) = \begin{bmatrix}
        1 & 2 & 0 & 5 & 0 \\
        0 & 0 & 1 & -1 & 0 \\
        0 & 0 & 0 & 0 & 1 \\
        0 & 0 & 0 & 0 & 0 
    \end{bmatrix} \).
    
    Applying the secret trick \dots

    A basis for \( \ima A \) is:
    \[ \left\{
        \begin{bmatrix} 1 \\ 3 \\ 1 \\ 2 \end{bmatrix}, \,
        \begin{bmatrix} 3 \\ 9 \\ 4 \\ 9 \end{bmatrix}, \,
        \begin{bmatrix} 3 \\ 2 \\ 2 \\ 2 \end{bmatrix}
    \right\} \]
    
    Recall \( \ima A = \left\{ c_1 \vec{v_1}+ c_2 \vec{v_2} + \dots + c_5 \vec{v_5} : c_i \in \mathbb{R} \right\} \).

    \[ \begin{matrix}
        \begin{bNiceMatrix}
            1 & 2 & 0 & 5 & 0 \\
            0 & 0 & 1 & -1 & 0 \\
            0 & 0 & 0 & 0 & 1 \\
            0 & 0 & 0 & 0 & 0
        \end{bNiceMatrix} \\
        \begin{matrix}
            -2 & 1 & 0 & 0 & 0 \\
            -5 & 0 & 1 & 1 & 0
        \end{matrix}
    \end{matrix}
    \]

    A basis for \( \ker A \) is:
    \[
        \left\{
            \begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \,
            \begin{bmatrix} -5 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}
        \right\}
     \]
    
    Recall \( \ker A =\left\{ \vec{c} \in \mathbb{R}^5 : A\vec{c} =0 \right\} \).
\end{example}

An important result regarding the kernel and image of a matrix is the rank-nullity theorem.

\begin{theorem}[Rank-Nullity Theorem]
    Let \( A \) be an \( m \times n \) matrix. Then \[ \dim(\ker A) + \dim(\ima A) = n \].
\end{theorem}

Another important result regards the dimension of a basis.

\begin{theorem}[]
    Let \( V \) be a vector space. If \( V \) has a finite basis, then any basis for \( V \) has the same number of vectors.
\end{theorem}

\begin{corollary}
    If \( \dim V = n \), then any set of \( n \) linearly independent vectors in \( V \) is a basis.
\end{corollary}

The secret trick gives us a method to find a basis and the dimension of the space. Then, any other vector can be expressed
using this basis.

\end{document}